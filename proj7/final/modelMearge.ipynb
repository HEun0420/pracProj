{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38eb3902b7d34d1f8eba27dc1b49e387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\201\\miniforge3\\envs\\pystudy\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\201\\.cache\\huggingface\\hub\\models--unsloth--Llama-3.2-1B-Instruct-bnb-4bit. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1e9359305b4c809f883f0ba6b6c358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa65fb5decc7464181e8bb515a922ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/90.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\201\\miniforge3\\envs\\pystudy\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\201\\.cache\\huggingface\\hub\\models--haeun0420--gorani-1B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\201\\miniforge3\\envs\\pystudy\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA 병합 완료 및 4bit 모델 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# 모델명 설정\n",
    "base_model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"  \n",
    "lora_model_name = \"haeun0420/gorani-1B\"\n",
    "\n",
    "# 1️⃣ 기본 모델 (Base Model) 먼저 불러오기\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "\n",
    "# 2️⃣ LoRA 모델 불러오기 및 병합\n",
    "lora_model = PeftModel.from_pretrained(base_model, lora_model_name)\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# 3️⃣ 병합된 모델 저장\n",
    "merged_model.save_pretrained(\"merged_llama3_1B_4bit\")\n",
    "\n",
    "print(\"✅ LoRA 병합 완료 및 4bit 모델 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face 토큰을 입력하여 로그인\n",
    "login(token=\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/haeun0420/gorani-1B-4bit', endpoint='https://huggingface.co', repo_type='model', repo_id='haeun0420/gorani-1B-4bit')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "# 원하는 모델 리포지토리 이름을 지정 (자신의 사용자명/모델명 형식으로)\n",
    "repo_name = \"haeun0420/gorani-1B-4bit\"\n",
    "\n",
    "# 리포지토리 생성\n",
    "create_repo(repo_name, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b32cb24faa45a88e15f5d735fd013f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/haeun0420/gorani-1B-4bit/commit/a44b0e4dabca34f84e62265e2eb975025f6dfbb1', commit_message='Upload folder using huggingface_hub', commit_description='', oid='a44b0e4dabca34f84e62265e2eb975025f6dfbb1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/haeun0420/gorani-1B-4bit', endpoint='https://huggingface.co', repo_type='model', repo_id='haeun0420/gorani-1B-4bit'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login, upload_folder\n",
    "\n",
    "\n",
    "# 모델 업로드\n",
    "upload_folder(repo_id=\"haeun0420/gorani-1B-4bit\", folder_path=\"merged_llama3_1B_4bit\")\n",
    "\n",
    "\n",
    "from huggingface_hub import upload_folder\n",
    "\n",
    "# 업로드할 모델 폴더 경로\n",
    "model_path = \"merged_llama3_1B_4bit\"\n",
    "\n",
    "# Hugging Face에 업로드할 리포지토리 이름 지정\n",
    "repo_name = \"haeun0420/gorani-1B-4bit\"\n",
    "\n",
    "# Hugging Face에 모델 업로드\n",
    "upload_folder(\n",
    "    repo_id=repo_name,\n",
    "    folder_path=model_path,\n",
    "    path_in_repo=\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
